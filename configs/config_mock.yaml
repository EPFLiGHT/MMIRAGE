processors:
  - type: llm
    server_args:
      model_path: Qwen/Qwen3-4B-Instruct-2507
      tp_size: 4          # use all 4 GPUs on the node
      disable_custom_all_reduce: true
    sampling_params:
      temperature: 0.1
      top_p: 0.9
      max_new_tokens: 1024
      custom_params:
        chat_template_kwargs: 
          enable_thinking: false

loading_params:
  datasets:
    - path: tests/mock_data/data.jsonl
      type: JSONL
      output_dir: tests/output/data
    - path: 
        train: tests/mock_data/data2/train.jsonl
        test: tests/mock_data/data2/test.jsonl
      type: JSONL
      output_dir: tests/output/data2

  num_shards: 4
  shard_id: 0
  conversations_field: "conversations"
  batch_size: 64

processing_params:
  inputs:
    - name: text
      key: text

  outputs:
    - name: formatted_answer
      type: llm
      output_type: JSON
      output_schema:
        - question
        - answer
      prompt: |
        Generate one question and its corresponding answer using the following text:
        ```
        {{ text }}
        ```
  
  remove_columns: True
  output_schema:
    conversations:
      - role: "user"
        content: "{{ formatted_answer.question }}"
      - role: "assistant"
        content: "{{ formatted_answer.answer }}"
