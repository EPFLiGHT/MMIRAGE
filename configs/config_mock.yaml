engine:
  model_path: Qwen/Qwen3-4B-Instruct-2507
  tp_size: 1          # use all 4 GPUs on the node
  trust_remote_code: true

sampling_params:
  temperature: 0.1
  top_p: 1.0
  max_new_tokens: 384
  custom_params:
    chat_template_kwargs: 
      enable_thinking: false

processing_gen_params:
  datasets:
    - path: tests/mock_data/data.jsonl
      type: JSONL
  output_dir: tests/output
  # num_shards: "$SLURM_ARRAY_TASK_COUNT"
  # shard_id: "$SLURM_ARRAY_TASK_ID"
  num_shards: 4
  shard_id: 0
  conversations_field: "conversations"
  batch_size: 64

processing_params:
  inputs:
    - name: text
      key: text

  outputs:
    - name: formatted_answer
      type: llm
      output_type: JSON
      output_schema:
        - question
        - answer
      prompt: |
        Generate one question and its corresponding answer using the following text:
        ```
        {text}
        ```

  output_schema:
    conversations:
      - role: "user"
        content: "{{ formatted_answer.question }}"
      - role: "assistant"
        content: "{{ formatted_answer.answer }}"
