engine:
  model_path: "Qwen/Qwen2.5-14B-Instruct" # "Qwen/Qwen3-Next-80B-A3B-Instruct"
  tp_size: 4          # use all 4 GPUs on the node
  trust_remote_code: true
  # Choose a KV cache + quantization that fits:
  # kv_cache_dtype: "fp8_e4m3"
  # quantization: "compressed-tensors"   # or "awq/gptq" depending on your setup

sampling_params:
  temperature: 0.0
  top_p: 1.0
  max_new_tokens: 2048
