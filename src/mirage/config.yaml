engine:
  model_path: "Qwen/Qwen3-Next-80B-A3B-Instruct"
  tp_size: 4          # use all 4 GPUs on the node
  trust_remote_code: true

sampling_params:
  temperature: 0.1
  top_p: 1.0
  max_new_tokens: 384

processing_gen_params:
  datasets:
    - /capstor/store/cscs/swissai/a127/meditron/multimediset/arrow/medtrinity_conversations_1/
    - /capstor/store/cscs/swissai/a127/meditron/multimediset/arrow/medtrinity_conversations_2/
  output_dir: /capstor/store/cscs/swissai/a127/homes/$USER/datasets/medtrinity/shards
  num_shards: "$SLURM_ARRAY_TASK_COUNT"
  shard_id: "$SLURM_ARRAY_TASK_ID"
  conversations_field: "conversations"
  batch_size: 64

processing_params:
  inputs:
    - name: assistant_answer
      key: conversations[1].content
    - name: user_prompt
      key: conversations[0].content
    - name: modalities
      key: modalities

  outputs:
    - name: formatted_answer
      type: llm
      output_type: plain
      prompt: | 
        Reformat the answer in a markdown format without adding anything else:
        {assistant_answer}
      output_schema:
        - question
        - explanation
        - answer
        
  output_schema:
    conversations:
      - role: user
        content: "{user_prompt}"
      - role: assistant
        content: "{formatted_answer}"
    modalities: "{modalities}"

