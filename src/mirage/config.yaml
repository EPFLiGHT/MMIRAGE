engine:
  model_path: "Qwen/Qwen3-Next-80B-A3B-Instruct"
  tp_size: 4          # use all 4 GPUs on the node
  trust_remote_code: true

sampling_params:
  temperature: 0.0
  top_p: 1.0
  max_new_tokens: 2048

processing_gen_params:
  datasets:
    - "/path/to/dataset1"
    - "/path/to/dataset2"
  output_dir: "/path/to/output"
  num_shards: 8
  shard_id: 0
  conversations_field: "conversations"
  batch_size: 64

processing_params:
  inputs:
    - name: assistant_answer
      key: conversations[1].content
    - name: user_prompt
      key: conversations[0].content
    - name: modalities
      key: modalities

  outputs:
    - name: formatted_answer
      type: llm
      output_type: plain
      prompt: | 
        Reformat the answer in a markdown format without adding anything else:
        {assistant_answer}
      output_schema:
        - question
        - explanation
        - answer
        
  output_schema:
    conversations:
      - role: user
        content: "{user_prompt}"
      - role: assistant
        content: "{formatted_answer}"
    modalities: "{modalities}"